{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Kaggle \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Kaggle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Founded in 2010, [Kaggle](www.Kaggle.com) is a Data Science platform where users can share, collaborate, and compete. One key feature of Kaggle is \"Competitions\", which offers users the ability to practice on real world data and to test their skills with, and against, an international community. \n",
    "\n",
    "This guide will teach you how to approach and enter a Kaggle competition, including exploring the data, creating and engineering features, building models, and submitting predictions.  We'll use [Python 3](https://www.python.org/) and [Jupyter Notebook](http://jupyter.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work through the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.\n",
    "\n",
    "\n",
    "\n",
    "We'll follow these steps to a successful Kaggle Competition submission:\n",
    "- Acquire the data\n",
    "- Explore the data\n",
    "- Engineer and transform the features and the target variable\n",
    "- Build a model\n",
    "- Make and submit predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](kaggle.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Acquire the data, create our environment\n",
    "We need to acquire the [data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) for the competition. The descriptions of the features and some other helpful information are contained in a file with an obvious name, `data_description.txt`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data and save it into a folder where you'll keep everything you need for the competition. \n",
    "\n",
    "We will first look at the `train.csv` data. After we've trained a model, we'll make predictions using the `test.csv` data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import [Pandas](http://www.dataquest.io/blog/pandas-python-tutorial/), a fantastic library for working with data in Python. Next we'll import [Numpy](https://www.dataquest.io/blog/numpy-tutorial-python/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pandas to read in csv files. The `pd.read_csv()` method creates a DataFrame from a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the size of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train data shape:\", train.shape)\n",
    "print (\"Test data shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `test` has only 80 columns, while `train` has 81. This is due to, of course, the fact that the test data do not include the final sale price information!\n",
    "\n",
    "Next, we'll look at a few rows using the `DataFrame.head()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have the `data dictionary` available in our folder for the competition. You can also find it [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). \n",
    "\n",
    "\n",
    "Here's a brief version of what you'll find in the data description file: \n",
    "\n",
    "- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "- MSSubClass -  The building class\n",
    "- MSZoning -  The general zoning classification\n",
    "- LotFrontage -  Linear feet of street connected to property\n",
    "- LotArea -  Lot size in square feet\n",
    "- Street -  Type of road access\n",
    "- Alley -  Type of alley access\n",
    "- LotShape -  General shape of property\n",
    "- LandContour - Flatness of the property\n",
    "- Utilities - Type of utilities available\n",
    "- LotConfig -  Lot configuration\n",
    "\n",
    "And so on. \n",
    "\n",
    "The competition challenges you to predict the final price of each home.\n",
    "At this point, we should start to think about what we know about housing prices, [Ames, Iowa](https://en.wikipedia.org/wiki/Ames,_Iowa), and what we might expect to see in this dataset. \n",
    "\n",
    "Looking at the data, we see features we expected, like `YrSold` (the year the home was last sold) and `SalePrice`. Others we might not have aniticipated, such as `LandSlope` (the slope of the land the home is built upon) and `RoofMatl` (the materials used to construct the roof). Later, we'll have to make decisions about how we'll approach these and other features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do some plotting during the exploration stage of our project, and we'll need to import that functionality into our environment as well. Plotting allows us to visualize the distribution of the data, check for outliers, and see other patterns that we might miss otherwise. We'll use [Matplotlib](http://matplotlib.org/), a  popular visualization library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2: Explore the data and engineer Features\n",
    "### The Target Variable\n",
    "What are we predicting?\n",
    "The challenge is to predict the final sale price of the homes. This information is stored in the `SalePrice` column. We can use `Series.describe()` to get more information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.SalePrice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `Series.describe()` to get more information about a series. `count` displays the total number of rows in the series. For numerical data, `Series.describe()` also gives the `mean` of the values. Other metrics, such as the standard deviation `std`, the `min` and `max` values, are included as well. \n",
    "\n",
    "The average sale price of a house in our dataset is close to `$180,000`, with most of the values falling within the `$130,000` to `$215,000` range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check for [skewness](http://mathworld.wolfram.com/Skewness.html) and look at the distribution.\n",
    "When performing regression, sometimes it makes sense to log-transform the target (dependent) variable when it is skewed. One reason for this is to improve the linearity of the data. Although the justification is beyond the scope of this tutorial, more information can be found [here](https://en.wikipedia.org/wiki/Data_transformation_%28statistics%29).\n",
    "\n",
    "Importantly, the predictions generated by the final model will also be log-transformed, so we'll need to convert these predictions back to their original form later. \n",
    "\n",
    "`np.log()` will transform the variable, and `np.exp()` will reverse the transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `plt.hist()` to plot a histogram of `SalePrice`. Notice that the distribution has a longer tail on the right. The distribution is positively skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Skew is:\", train.SalePrice.skew())\n",
    "plt.hist(train.SalePrice, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `np.log()` to transform `train.SalePrice`. We will print the skewness a second time. A value closer to 0 means that we have improved the skewness of the data. We expect that the data will now resemble a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(train.SalePrice)\n",
    "print (\"Skew is:\", target.skew())\n",
    "plt.hist(target, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've transformed the target variable, let's consider our features. First, we'll check out the numerical features and make some plots. The `.select_dtypes()` method will return a subset of columns matching the specified data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train.select_dtypes(include=[np.number])\n",
    "numeric_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `DataFrame.corr()` displays the correlations (or relationships) between the columns. We'll examine the correlations between the features and the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = numeric_features.corr()\n",
    "\n",
    "print (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\n",
    "print (corr['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five features are the most [positively correlated](https://en.wikipedia.org/wiki/Correlation_and_dependence) with `SalePrice`, while the next five are the most negatively correlated. \n",
    "\n",
    "Let's dig deeper on `OverallQual`. We can use the `.unique()` method to get the unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.OverallQual.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OverallQual` data are integer values in the interval 1 to 10 inclusive. \n",
    "\n",
    "We can create a [pivot table](https://en.wikipedia.org/wiki/Pivot_table) to further investigate the relationship between `OverallQual` and `SalePrice`. The [Pandas docs](http://pandas.pydata.org/pandas-docs/stable/reshaping.html) demonstrate how to accomplish this task. We set `index='OverallQual' ` and `values='SalePrice' `. We chose to look at the `median` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us visualize this pivot table more easily, we can create a bar plot. We can use the `Series.plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_pivot.plot(kind='bar', color='blue')\n",
    "plt.xlabel('Overall Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the median sales price strictly increases as Overall Quality increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `plt.scatter()` to generate scatter plots. We want to visualize the relationship between the Ground Living Area `GrLivArea` and `SalePrice`. We will do the same for `GarageArea`. At first glance, we see that increases in living area correspond to increases in price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=train['GrLivArea'], y=target)\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('Above grade (ground) living area square feet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=train['GarageArea'], y=target)\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('Garage Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are many homes with `0` for `Garage Area`, indicating that they don't have a garage. We'll transform other features later to reflect this assumption. There are a few [outliers](https://en.wikipedia.org/wiki/Outlier) as well. Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line. So, we'll remove those observations from our data. Removing outliers is an art and a science. There are many techniques for dealing with outliers.  \n",
    "\n",
    "We will create a new dataframe with some outliers removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['GarageArea'] < 1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('Garage Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "Next, we'll examine the null or missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a DataFrame to view the top null columns. Chaining together the `train.isnull().sum()` methods, we return a Series of the counts of the null values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])\n",
    "nulls.columns = ['Null Count']\n",
    "nulls.index.name = 'Feature'\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [documentation](https://kaggle2.blob.core.windows.net/competitions-data/kaggle/5407/data_description.txt?sv=2015-12-11&sr=b&sig=AC7RwCFI7%2FFpO1BuiW91n1d6LWJBxzLkqeprqICtC%2FY%3D&se=2017-04-30T12%3A56%3A21Z&sp=r) can help us understand the missing values. In the case of PoolQC, the column refers to Pool Quality.  Pool quality is NaN when PoolArea is 0, or there is no pool.\n",
    "We can find a similar relationship between many of the Garage-related columns. \n",
    "\n",
    "\n",
    "Let's take a look at one of the other columns, `MiscFeature`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Series.unique()` method to return a list of the unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Unique values are:\", train.MiscFeature.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation decodes these. \n",
    "\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "\n",
    "       Elev Elevator\n",
    "       Gar2 2nd Garage (if not described in garage section)\n",
    "       Othr Other\n",
    "       Shed Shed (over 100 SF)\n",
    "       TenC Tennis Court\n",
    "       NA   None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn that these values describe whether or not the house has a shed over 100 sqft, a second garage, and so on. We might want to use this information later. It's important to gather domain knowledge in order to make the best decisions when dealing with missing data.\n",
    "\n",
    "\n",
    "### Non-numeric Features\n",
    "Let's now consider the non-numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals = train.select_dtypes(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.T` will display the [transpose matrix](https://en.wikipedia.org/wiki/Transpose). In this case, it's easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals.describe().T.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` column indicates the count of not null observations, while `unique` counts the number fof unique values. `top` is the most commonly occurring value, with frequency equal to `freq`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of these features, we might want to use [one-hot encoding](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) to make use of the information for modelling. \n",
    "One-hot encoding is a technique which will transform categorical data into numbers so the model can understand whether or not a particular observation falls into one category or another. \n",
    "\n",
    "### Transforming and engineering features\n",
    "\n",
    "Importantly, remember that any transformations that you've applied to the training data before fitting the model *must* be applied to the test data. You must include this step. Our model expects that the shape of the features from the `train` set match those from the `test` set. This means that any feature engineering that occurred while working on the `train` data should be applied again on the `test` set. Since we'll encode information from the `SaleCondition` and `Street` columns in the `trian` data, we must do apply this to the `test` data. \n",
    "\n",
    "\n",
    "To demonstrate how this works, consider the `Street` data, which indicates whether there is `Gravel` or `Paved` road access to the property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Original: \\n\") \n",
    "print (train.Street.value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Street` column, the unique values are `Pave` and `Grvl`, which describe the type of road access to the property. In the training set, only 5 homes have gravel access. Our model needs numerical data, so we will use one-hot encoding to transform the data into a Boolean column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column called `enc_street`. The `pd.get_dummies()` method will handle this for is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\n",
    "test['enc_street'] = pd.get_dummies(train.Street, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Encoded: \\n') \n",
    "print (train.enc_street.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values agree. We've engineered our first feature! [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering) is the process of making features of the data suitable for use in machine learning and modelling. When we encoded the `Street` feature into a column of Boolean values, we engineered a feature. \n",
    "\n",
    "Let's try engineering another feature. We'll look at `SaleCondition` by constructing and plotting a pivot table, as we did above for `OverallQual`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\n",
    "condition_pivot.plot(kind='bar', color='blue')\n",
    "plt.xlabel('Sale Condition')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Partial` has a significantly higher Median Sale Price than the others. We will encode this as a new feature, following a similar method we used for `Street` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['enc_condition'] = train.SaleCondition.apply(lambda x: 1 if x=='Partial' else 0)\n",
    "test['enc_condition'] = test.SaleCondition.apply(lambda x: 1 if x=='Partial' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this new feature as a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)\n",
    "condition_pivot.plot(kind='bar', color='blue')\n",
    "plt.xlabel('Encoded Sale Condition')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great. Continue to work with more features to improve the ultimate performance of your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we prepare the data for modeling, we will deal with the missing data. Our strategy fills the missing values with an average value and then assign the results to `data`. This is a method of [interpolation](https://en.wikipedia.org/wiki/Interpolation). The `DataFrame.interpolate()` method makes this simple. \n",
    "Thoughtful handling of missing values can improve the accuracy of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = train.select_dtypes(include=[np.number]).interpolate().dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the all of the columns have 0 null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.isnull().sum() != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Build a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select the target variable, `y`, and the predictor varibales `X`.  Use `DataFrame.drop()` to exclude the target column, `SalePrice`, and the `Id` column. We drop the `Id` column because this is an index and is not a feature of our observations. \n",
    "\n",
    "### Partition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.log(train.SalePrice)\n",
    "X = data.drop(['SalePrice', 'Id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's partition the data and start modelling. \n",
    "We will use the `train_test_split()` function from [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create a training set and a hold-out set. Partitioning the data in this way allows us to evaluate how our model might perform on data that it has never seen before. If we train the model on all of the test data, it will be diffucult to tell if [overfitting](https://en.wikipedia.org/wiki/Overfitting) has taken place. \n",
    "\n",
    "`train_test_split()` returns four objects:\n",
    "- `X_train` is the subset of our features used for training\n",
    "- `X_test` is the subset which will be our 'hold-out' set, what we'll use to test the model\n",
    "- `y_train` is the subset of Sale Prices which correspond to `X_train`\n",
    "- `y_test` is the subset of Sale Prices which correspond to `X_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter value `X` denotes the set of predictor data, and `y` is the target variable. Next, we set `random_state=42`. This provides for reproducible results, since sci-kit learn's `train_test_split` will randomly partition the data. The `test_size` parameter tells the function what proportion of the data should be in the `test` partition. In this example, about 33% of the data is devoted to the hold-out set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin modelling\n",
    "\n",
    "We will first create a [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) model. Step one is always to instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to fit the model. First instatiate the model and next fit the model. Model fitting is a procedure that varies for different types of models. Put simply, we are estimating the relationship between our predictors and the target variable so we can make accurate predictions on new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model using `X_train` and `y_train`, and we'll score with `X_test` and `y_test`. The `lr.fit()` method will fit the linear regression on the features and target variable that we pass. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance and visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to evaluate the performance of the model. \n",
    "Each competition might [evaluate](https://www.kaggle.com/c/house-prices-advanced-regression-techniques#evaluation) the submissions differently. In this competition, Kaggle will evaluate our submission using [root-mean-squared-error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation). We'll also look at The [r-squared value](https://en.wikipedia.org/wiki/Coefficient_of_determination). The r-sqaured value is a measure of how close the data are to the fitted regression line. It takes a value between 0 and 1, 1 meaning that all of the variance in the target is explained by the data. In general, a higher r-squared value means a better fit. \n",
    "\n",
    "The `model.score()` method returns the r-squared value by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"R^2 is: \\n\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our features explain approximately 86% of the variance in our target variable. Follow the link above to learn more. \n",
    "\n",
    "Next, we'll consider `rmse`. To do so, we need to create a list of our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict()` method will return a list of predictions given a set of predictors. Use `model.predict()` after fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print ('RMSE is: \\n', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean_squared_error` function takes two arrays and calculates the `rmse`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting this value is somewhat more intuitive that the r-squared value. The RMSE measures the distance between our predicted values and actual values. \n",
    "\n",
    "We can view this relationship graphically with a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_test\n",
    "plt.scatter(predictions, actual_values, alpha=.75, color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our predicted values were identical to the actual values, this graph would be the straight line `y=x` because each predicted value `x` would be equal to each actual value `y`. \n",
    "\n",
    "### Try to improve the model\n",
    "\n",
    "We'll next try using [Ridge Regularization](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) to decrease  the influence of less important features. Ridge Regularization is a process which shrinks the regression coefficients of less important features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again instantiate the model. The Ridge Regularization model takes a parameter, `alpha` ,  which controls the strength of the regularization. Experiment with different values for alpha and see how this changes your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm = linear_model.Ridge(alpha=.030)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the model and make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_model = rm.fit(X_train, y_train)\n",
    "preds_ridge = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute and compare the scores. Next, we'll create another scatter plot of our predicted values v actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "print (\"R^2 is: \\n\", ridge_model.score(X_test, y_test))\n",
    "print ('RMSE is: \\n', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds_ridge, actual_values, alpha=.75, color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Ridge Regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop through a few different values of alpha, following the same instantiate-fit-predict procedure above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (-2, 3):\n",
    "    alpha = 10**i\n",
    "    rm = linear_model.Ridge(alpha=alpha)\n",
    "    ridge_model = rm.fit(X_train, y_train)\n",
    "    preds_ridge = ridge_model.predict(X_test)\n",
    "    #compute r score\n",
    "    print ('alpha is: ', alpha )\n",
    "    print (\"R^2 is: \\n\", ridge_model.score(X_test, y_test)) #compute the R^2 score\n",
    "    print ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n",
    "    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b') #alpha helps to show overlapping data\n",
    "    plt.xlabel('Predicted Price')\n",
    "    plt.ylabel('Actual Price')\n",
    "    plt.title('Ridge Regularization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models perform almost identically to the first model. In our case, adjusting the alpha did not substantially improve our model. As you add more features, regularization can be helpful. Repeat this step after you've added more features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make a submission\n",
    "We'll need to create a `csv` that contains the predicted `SalePrice` for each observation in the `test.csv` dataset. \n",
    "\n",
    "We'll log in to our Kaggle account and go to the [submission page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit) to make a submission. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `DataFrame.to_csv()` to create a csv to submit.\n",
    "The first column must the contain the ID from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['Id'] = test.Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select the features from the test data for the model as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = test.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll transform the predictions to the correct form. Remember that to reverse `log()` we do `exp()`. \n",
    "So we will apply `np.exp()` to our predictions becasuse we have taken the logarithm previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Original predictions are:\", predictions[:5], \"\\n\")\n",
    "print (\"Final predictions are:\", final_predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['SalePrice'] = final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head() #check that everything looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we're confident that we've got the data arranged in the proper format, we can export to a `.csv file` as Kaggle expects. We pass `index=False` because Pandas otherwise would create a new index for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission1.csv', index=False) # prevents pandas from reindexing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit our results\n",
    "\n",
    "We've created a file called `submission1.csv` in our working directory that conforms to the correct format. Go to the [submission page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit) to make a submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text]('submission1_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We placed 1602 out of about 2400 competitors. Almost middle of the pack, not bad! Notice that our score here is `.15097`, which is better than the score we observed on the test data. That's a good result, but will not always be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "The workflow for this submission:\n",
    "- Acquired the data for the competition, set up our environment\n",
    "- Explored the data and engineered the features\n",
    "- Built and fit a linear model\n",
    "- Generated predictions using the test data\n",
    "- Made a successful submission\n",
    "\n",
    "## Next steps\n",
    "We created a simple model. Improve your results by:\n",
    "- Working with  and transforming other features in the training set\n",
    "- Experimenting with different modelling techniques, such as Random Forest Regressors or Gradient Boosting\n",
    "- Using [ensembling models](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "\n",
    "We created a set of categorical features  called `categoricals` that were not all included in the final model. Go back and try to include these features. There are other methods that might help with categorical data, notably the `pd.get_dummies()` method. After working on these features, repeat the transformations for the test data and make another submission. \n",
    "\n",
    "Working on models and participating in Kaggle competitions can be an iterative process -- it's important to experiment with new ideas, learn about the data, and test newer models and techniques. \n",
    "\n",
    "With these tools, you can build upon your work and improve your results. \n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
